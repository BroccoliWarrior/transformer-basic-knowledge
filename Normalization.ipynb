{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP78+Gld1BmBTXslaxvj+3D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BroccoliWarrior/transformer-basic-knowledge/blob/main/Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Normalization***\n",
        "\n",
        "purpose：标准化隐藏层的输出，稳定训练过程，缓解梯度消失或爆炸的问题，同时加速收敛并提高模型的泛化能力\n",
        "\n"
      ],
      "metadata": {
        "id": "PYsRGUV0dw-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***几种常见的Normalization***\n",
        "\n",
        "    * Batch Normalization\n",
        "    * Layer Normalization\n",
        "    * Instance Normalization\n",
        "    * Group Normalization"
      ],
      "metadata": {
        "id": "I_SlnQK7eOLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Batch Normalization**\n",
        "\n",
        "  在激活函数之前，对每个channel在batch维度上计算均值与方差，并将激活值约束到均值为0、方差为1的分布，从而***减少ICS问题，并加速收敛***\n",
        "\n",
        "    * ICS（Internal Covariate Shift）：随着训练的进行，网络参数不断更新，这会导致每一层的输入数据的分布发生改变。例如，在神经网络的第一层输入数据可能服从某种分布，但是经过第一层的变换后，第二层的输入数据分布就会发生变化，而且这种变化会随着网络层数的增加不断累积\n",
        "    * 这种输入分布的变化会使得模型训练变得困难\n",
        "\n",
        "  **计算过程**\n",
        "\n",
        "  假设某一层的输出维度为[N,C,H,W]，其中N表示batch_size，C表示通道数，H和W分别表示特征图的高和宽\n",
        "\n",
        "  step1：计算均值\n",
        "\n",
        "  $\\mu_c = \\frac{1}{N \\times H \\times W} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{n,c,h,w}$\n",
        "\n",
        "  step2：计算方差\n",
        "\n",
        "  $\\sigma_c^2 = \\frac{1}{N \\times H \\times W} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{n,c,h,w} - \\mu_c)^2$\n",
        "\n",
        "  step3：归一化\n",
        "\n",
        "  $\\hat{x}_{n,c,h,w} = \\frac{x_{n,c,h,w} - \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}}$\n",
        "\n",
        "  step4：缩放和平移\n",
        "\n",
        "  $y_{n,c,h,w} = \\gamma_c \\hat{x}_{n,c,h,w} + \\beta_c$\n",
        "\n",
        "  其中 $\\gamma_c$ 和 $\\beta_c$ 为可学习参数（初始时常设为 $\\gamma_c = 1$，$\\beta_c = 0$）\n",
        "\n",
        "  **作用**\n",
        "\n",
        "    * 减少ICS，避免梯度消失或爆炸\n",
        "    * 允许使用较大的学习率\n",
        "    * 降低对权重初始化的敏感度\n",
        "    * 对隐藏层输出有轻微的正则化效果（在训练过程中，BN 是基于一个 mini-batch 的数据来计算均值和方差进行归一化操作的。由于不同 mini-batch 的数据存在差异，这就导致每次归一化的结果会有一定的波动。对于网络中的某一层来说，这种波动类似于给输入数据加入了噪声。模型为了适应这种噪声，就需要学习更加鲁棒的特征表示，从而提高了模型的泛化能力 ，起到了类似正则化的效果。）"
      ],
      "metadata": {
        "id": "y_0QEABDeceQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  2. **Layer Normalization**\n",
        "\n",
        "  不依赖batch维度，在单个样本内对特征进行归一化\n",
        "\n",
        "  **计算过程**\n",
        "\n",
        "  给定输入向量$x=(x_1,x_2,⋯,x_n)$，LN统计所有元素的均值与方差\n",
        "\n",
        "  $y = \\frac{x - \\text{E}(x)}{\\sqrt{\\text{Var}(x) + \\epsilon}} * \\gamma + \\beta$\n",
        "\n",
        "  其中$\\text{E}(x) = \\frac{1}{n} \\sum_{i=1}^{n} x_i$，$\\text{Var}(x) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\text{E}(x))^2$\n",
        "\n",
        "    * 这里是“有偏估计”"
      ],
      "metadata": {
        "id": "iY-2I7wmjLaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, features, eps=1e-6) -> None:\n",
        "    super(LayerNorm, self).__init__()\n",
        "\n",
        "    self.gamma = nn.Parameter(torch.ones(features))\n",
        "    self.beta = nn.Parameter(torch.zeros(features))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    std = x.std(-1, keepdim=True, unbiased=False)\n",
        "    return self.gamma * (x - mean) / (std + self.eps) + self.beta"
      ],
      "metadata": {
        "id": "EcOVQaTmop_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  3. **Instance Normalization**\n",
        "\n",
        "  IN最初用于图像风格迁移任务，在每张图像的特征图上分别做归一化\n",
        "\n",
        "  **计算过程**\n",
        "\n",
        "  对输入$x∈\\mathbb {R} ^{N×C×H×W}$，IN在每个样本n、每个通道c的特征图上计算均值和方差\n",
        "\n",
        "  step1：计算均值\n",
        "\n",
        "  $\\mu_{n,c} = \\frac{1}{HW} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{n,c,h,w}$\n",
        "\n",
        "  step2：计算方差\n",
        "\n",
        "  $\\quad \\sigma_{n,c}^2 = \\frac{1}{HW}\\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{n,c,h,w} - \\mu_{n,c})^2$\n",
        "\n",
        "  step3：归一化\n",
        "\n",
        "  $y_{n,c,h,w} = \\frac{x_{n,c,h,w} - \\mu_{n,c}}{\\sqrt{\\sigma_{n,c}^2 + \\epsilon}} \\quad $"
      ],
      "metadata": {
        "id": "TLUCq1xGkWpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  4. **Group Normalization**\n",
        "\n",
        "  主要为了**解决BN对batch size依赖较大**\n",
        "\n",
        "  **计算过程**\n",
        "\n",
        "  将$C$个通道，分为$G$组，则每组有$C/G$个通道，在单个样本的特征图中，对同一组的所有通道的其对于的空间位置$H×W$做均值和方差计算\n",
        "\n",
        "  $\\mu_{n,g} = \\frac{1}{(C/G)HW} \\sum_{c=g\\frac{C}{G}}^{(g+1)\\frac{C}{G} - 1} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{n,c,h,w}$\n",
        "\n",
        "  $\\sigma_{n,g} = \\sqrt{\\frac{1}{(C/G)HW} \\sum_{c=g\\frac{C}{G}}^{(g+1)\\frac{C}{G} - 1} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{n,c,h,w} - \\mu_{n,g})^2 + \\epsilon}$"
      ],
      "metadata": {
        "id": "UDGEBg4xmBK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Layer Normalization的优化方案***\n",
        "\n",
        "1. Root Mean Square（RMS） Layer Normalization\n",
        "\n",
        "  与LN对比起来，RMS Norm不需要进行$x-μ$，只用：\n",
        "\n",
        "  $\\text{RMS}(\\mathbf{a}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} a_i^2}$\n",
        "\n",
        "  $\\bar{a}_i = \\frac{a_i}{\\text{RMS}(\\mathbf{a})} \\cdot g_i$\n",
        "\n",
        "  其中$g_i$是可学习的缩放系数\n",
        "\n",
        "2. pRMS Norm\n",
        "\n",
        "  RMS Norm对整个向量都要计算$RMS$，当面对大向量或特指数极多的情况，计算量增大，于是提出pRMS Norm，仅使用前$p%$的元素计算$RMS$"
      ],
      "metadata": {
        "id": "gnCdZAXbnVeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Layer Normalization和Instance Normalization***\n",
        "\n",
        "  1. **区别**\n",
        "\n",
        "  * LayerNorm 在**单个样本内部的特征维**上做标准化\n",
        "  * InstanceNorm 在**单个样本的每个通道的空间位置**上做标准化\n",
        "\n",
        "  **Layer Norm**：假设 N=1, L=3, D=4，输入形状为 (1, 3, 4)：\n",
        "\n",
        "    token1: [1.0, 2.0, 3.0, 4.0]\n",
        "\n",
        "    token2: [2.0, 4.0, 6.0, 8.0]\n",
        "  \n",
        "    token3: [1.5, 3.0, 4.5, 6.0]\n",
        "\n",
        "  对每个 token 的 4 维向量单独作为一个集合做归一化（每行单独处理），确保同一 token 内部的特征尺度一致\n",
        "\n",
        "  **Instance Norm**：假设 N=1, C=1, H=2, W=2，灰度图像，像素矩阵：\n",
        "\n",
        "    [[1.0, 2.0],\n",
        "    \n",
        "    [3.0, 4.0]]\n",
        "\n",
        "  把这张图该通道的 4 个像素作为一个集合做归一化，使得该通道内的像素分布在该图上被标准化\n"
      ],
      "metadata": {
        "id": "Yazrc-jvoYLq"
      }
    }
  ]
}