{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLr2jc30AgRVoGrptqHpxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BroccoliWarrior/transformer-basic-knowledge/blob/main/Positional_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Positional Encoding***\n",
        "\n",
        "Self-Attention没有内在的顺序概念，其在计算attention时，可以并行的关注序列中任意位置的符号或词，不像RNN按照时间展开或CNN局部扫描，而序列位置信息又是必要的，所以在Transformer中引入Positional Encoding\n",
        "\n",
        "\n",
        "\n",
        "**baseline**：给每个输入token添加一个向量，用于表示其在序列中的位置/顺序"
      ],
      "metadata": {
        "id": "murtVQdyFdQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "方法：\n",
        "* 绝对位置编码\n",
        "  * 固定的正余弦位置编码\n",
        "  * 可学习的位置编码\n",
        "* 相对位置编码\n",
        "  * 旋转位置编码\n",
        "  * ALiBi\n",
        "  "
      ],
      "metadata": {
        "id": "UTid1JybGgwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 方法                  | 实现方式                           | 是否可外推 | 参数量       |\n",
        "|-----------------------|------------------------------------|------------|--------------|\n",
        "| Sinusoidal            | 固定正余弦公式                     | 可以       | 无额外参数   |\n",
        "| Learnable Positional  | 训练一个可学习嵌入表               | 不可以     | 成比例增加   |\n",
        "| Relative Positional   | 基于 token 之间距离的可学习向量    | 视实现     | 需要额外参数 |\n",
        "| RoPE (旋转位置编码)   | 对 Q/K 做旋转变换引入位置信息      | 可以       | 无额外参数   |\n",
        "| ALiBi (线性偏置)      | 在注意力打分时添加线性距离偏置     | 可以       | 少量可训练   |\n",
        "\n"
      ],
      "metadata": {
        "id": "ouf1jXLrG1VE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. 位置编码与词向量的加法/拼接方式有什么区别？\n",
        "\n",
        "    a.相加最为简单，不会改变原有的维度\n",
        "  \n",
        "    b.拼接后需要调大下游网络的维度\n",
        "\n",
        "2. 序列长度非常大怎么办？\n",
        "\n",
        "    a.固定正余弦可以外推，但可能存在数值不稳定或分辨率不足的问题\n",
        "    b.如果采用可学习的embedding，超大长度会带来巨大的参数开销\n",
        "    c.RoPE或ALiBi在大多数模型中表现良好\n",
        "\n",
        "3. 为什么正余弦有利于插值和外推？\n",
        "\n",
        "    a.正余弦是周期函数，模型可以学习到它们的周期模式，并在未见过的位置上应用这种周期性\n",
        "\n",
        "4. 多头注意力Muti-Attention下，每个头的位置信息如何处理？\n",
        "\n",
        "    a.位置编码一般与输入向量在词向量维度同一空间，因此对多头注意力无影响\n",
        "    b.多头注意力关注通道拆分后，位置编码已经注入到向量中"
      ],
      "metadata": {
        "id": "aLPCJCCQJ3DQ"
      }
    }
  ]
}