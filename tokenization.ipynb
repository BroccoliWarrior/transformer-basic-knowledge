{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY0N9tTpd8K+2bauaXYc+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BroccoliWarrior/transformer-basic-knowledge/blob/main/tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "正向最大匹配法FMM"
      ],
      "metadata": {
        "id": "Ye4vl4k2LwTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_max_matching(sentense, dictionary, max_len=6):\n",
        "  result = []\n",
        "  i = 0\n",
        "  while i < len(sentense):\n",
        "    matched = False\n",
        "    for j in range(max_len, 0, -1):\n",
        "      if i + j > len(sentense):\n",
        "        continue\n",
        "      word = sentense[i:i+j]\n",
        "      if word in dictionary:\n",
        "        result.append(word)\n",
        "        i += j\n",
        "        matched = True\n",
        "        break\n",
        "    if not matched:\n",
        "      result.append(sentense[i])\n",
        "      i += 1\n",
        "\n",
        "  return result\n",
        "\n",
        "dictionary = {'大模型', '学习', '模型', '我要', '要'}\n",
        "\n",
        "sentense = \"我要学习大模型\"\n",
        "\n",
        "print('/'.join(forward_max_matching(sentense, dictionary)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM-Ng-I0MIrn",
        "outputId": "166bc7bf-e3b6-425b-a4de-16efdacde1b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我要/学习/大模型\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "逆向最大匹配法BMM"
      ],
      "metadata": {
        "id": "HUbPK4yqOHki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_max_matching(sentense, dictionary, max_len=6):\n",
        "  result = []\n",
        "  i = len(sentense)\n",
        "  while i > 0:\n",
        "    matched = False\n",
        "    for j in range(max_len, 0, -1):\n",
        "      if i - j < 0:\n",
        "        continue\n",
        "      word = sentense[i-j:i]\n",
        "      if word in dictionary:\n",
        "        result.insert(0, word)\n",
        "        i -= j\n",
        "        matched = True\n",
        "        break\n",
        "    if not matched:\n",
        "      result.insert(0, sentense[i-1])\n",
        "      i -= 1\n",
        "\n",
        "  return result\n",
        "\n",
        "dictionary = {'大模型', '学习', '模型', '我要', '要'}\n",
        "\n",
        "sentense = \"我要学习大模型\"\n",
        "\n",
        "print('/'.join(backward_max_matching(sentense, dictionary)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG4_rR0VNraa",
        "outputId": "c1d575be-0e42-4e57-db47-7bcf77189716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我要/学习/大模型\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "根据**拆分粒度**的不同，Tokenization可以分为：\n",
        "1.   词粒度-Word-based\n",
        "2.   子词粒度-Subword-based\n",
        "3.   字符粒度-Character-based\n",
        "\n",
        "其中最常用的是**子词粒度**\n",
        "\n",
        "reason1.\n",
        "  词粒度：长尾效应和稀有词问题以及OOV\n",
        "reason2.\n",
        "  字符粒度：虽解决了OOV，但是处理效率低且丢失语义\n",
        "\n"
      ],
      "metadata": {
        "id": "PahNzfZrYgrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面介绍基于子词粒度的Tokenization方法：\n",
        "\n",
        "\n",
        "**BPE：Byte-Pair Encoding**\n",
        "\n",
        "  1.从 **字符级** 开始，把训练语料切分成最小单位（单个字符）\n",
        "\n",
        "  2.然后逐步 合并出现频率最高的相邻符号对（bigram），生成新的子词单元\n",
        "\n",
        "  3.直到 ***merge_rule*** 达到设定的 词表大小 (target_vocab_size) 或 合并次数 (merge operations)\n",
        "  \n",
        "  4.***利用merge_rule进行分词***"
      ],
      "metadata": {
        "id": "00GvG1X7cGpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "def get_stats(vocab):\n",
        "    \"\"\"计算当前词表中所有相邻符号对的出现次数\"\"\"\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[(symbols[i], symbols[i+1])] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, vocab):\n",
        "    \"\"\"合并最频繁的符号对\"\"\"\n",
        "    bigram = ' '.join(pair)\n",
        "    new_vocab = {}\n",
        "    for word in vocab:\n",
        "        new_word = word.replace(bigram, ''.join(pair))\n",
        "        new_vocab[new_word] = vocab[word]\n",
        "    return new_vocab\n",
        "\n",
        "def bpe(vocab, num_merges):\n",
        "    \"\"\"执行 BPE 算法\"\"\"\n",
        "    for i in range(num_merges):\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "        # 找到出现次数最多的符号对\n",
        "        best = max(pairs, key=pairs.get)\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "        print(f'Merge {i+1}: {best}, vocab: {vocab}')\n",
        "    return vocab\n",
        "\n",
        "# 示例文本\n",
        "corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
        "\n",
        "# 初始化词表：每个词拆分为字符，并加入空格分隔符，统计词频\n",
        "vocab = Counter()\n",
        "for word in corpus:\n",
        "    vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "\n",
        "print(\"Initial vocab:\", vocab)\n",
        "\n",
        "# 执行 BPE\n",
        "num_merges = 10\n",
        "vocab = bpe(vocab, num_merges)\n",
        "print(\"Final vocab:\", vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkfeQ2ZDYrBe",
        "outputId": "4685a639-c442-4134-b9da-576ee7939438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial vocab: Counter({'l o w </w>': 1, 'l o w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e s t </w>': 1})\n",
            "Merge 1: ('l', 'o'), vocab: {'lo w </w>': 1, 'lo w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e s t </w>': 1}\n",
            "Merge 2: ('lo', 'w'), vocab: {'low </w>': 1, 'low e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e s t </w>': 1}\n",
            "Merge 3: ('e', 's'), vocab: {'low </w>': 1, 'low e r </w>': 1, 'n e w es t </w>': 1, 'w i d es t </w>': 1}\n",
            "Merge 4: ('es', 't'), vocab: {'low </w>': 1, 'low e r </w>': 1, 'n e w est </w>': 1, 'w i d est </w>': 1}\n",
            "Merge 5: ('est', '</w>'), vocab: {'low </w>': 1, 'low e r </w>': 1, 'n e w est</w>': 1, 'w i d est</w>': 1}\n",
            "Merge 6: ('low', '</w>'), vocab: {'low</w>': 1, 'low e r </w>': 1, 'n e w est</w>': 1, 'w i d est</w>': 1}\n",
            "Merge 7: ('low', 'e'), vocab: {'low</w>': 1, 'lowe r </w>': 1, 'n e w est</w>': 1, 'w i d est</w>': 1}\n",
            "Merge 8: ('lowe', 'r'), vocab: {'low</w>': 1, 'lower </w>': 1, 'n e w est</w>': 1, 'w i d est</w>': 1}\n",
            "Merge 9: ('lower', '</w>'), vocab: {'low</w>': 1, 'lower</w>': 1, 'n e w est</w>': 1, 'w i d est</w>': 1}\n",
            "Merge 10: ('n', 'e'), vocab: {'low</w>': 1, 'lower</w>': 1, 'ne w est</w>': 1, 'w i d est</w>': 1}\n",
            "Final vocab: {'low</w>': 1, 'lower</w>': 1, 'ne w est</w>': 1, 'w i d est</w>': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google提出了***SentencePiece***风格的BPE，加速了该算法\n",
        "\n",
        "(1) 只维护活动 bigram（active bigram）\n",
        "\n",
        "Active bigram：当前词表中仍然存在的符号对（可能被合并）。\n",
        "\n",
        "Inactive bigram：已经被合并、不会再出现的 bigram。\n",
        "\n",
        "通过 **优先队列（heap）**维护 bigram 出现次数。\n",
        "\n",
        "每次只合并 频率最高的 active bigram → 不用每次全表扫描。\n",
        "\n",
        "(2) 增量更新频率\n",
        "\n",
        "合并某个 bigram 后，**只更新受影响的相邻 bigram 的计数**。\n",
        "\n",
        "不必每次重新统计整个词表。\n",
        "\n",
        "极大减少了重复计算量。\n",
        "\n",
        "(3) 优先队列 + 哈希表\n",
        "\n",
        "bigram 出现次数用哈希表维护。\n",
        "\n",
        "优先队列快速获取出现次数最多的 bigram。\n",
        "\n",
        "合并只影响局部词，所以更新复杂度低。"
      ],
      "metadata": {
        "id": "hHYv18sFdIqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BBPE：Byte-level BPE**\n",
        "\n",
        "文本 → UTF-8 字节序列 → BPE 合并 → 子词编码"
      ],
      "metadata": {
        "id": "useSO56resDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordPiece\n",
        "\n",
        "总体流程同BPE，但是挑选best-bigram的指标从频率变成了PMI（点互信息，Pointwise Mutual Information）：\n",
        "\n",
        "$\\mathrm{PMI}(a,b)=\\frac{P(a,b)}{P(a)P(b)}$，其中a，b是临近的subword\n",
        "\n",
        "分词方法：使用FMM进行vocabulary匹配"
      ],
      "metadata": {
        "id": "bDF6mr5pf3Mb"
      }
    }
  ]
}